#  -*- mode: org -*-
#+STARTUP: indent

* TODO 

Test for reallocating inode before shrunk

directories ls: if lock ordering problem, return what we have read so far

error codes
  write cnt = 0 should return which error happened

check that inodes are not modified on disk on abort
  e.g., nlink on failure to add to dir

mtime

performance measurements

end-to-end tests for recovery

goosify

log-by-pass writes

simplify allocator, plan 1
  goal: avoid locking regions; no search for free object as part of txn
    txn never issues reads to bitmaps (except at recovery time)
    txn contains single-bit writes to freed or allocated blocks
  in-memory allocator state: list of free object IDs (block#s, inode#s)
    probably map[uint64]struct{}
  invariant: in-memory allocator state (set of free object IDs) is a
    subset of the on-disk allocator state (set of free bits in bitmap)
  allocation returns one of the in-memory IDs, and marks it in-use
  after allocating an object ID, add txn write to the allocated bitmap bit
  if txn aborts, return allocated IDs to the in-memory allocator state
  when freeing object, add txn write to the freed bitmap bit
  after txn commits, put freed IDs into the in-memory allocator state
  on recovery, populate in-memory allocator state from on-disk bitmap

simplify allocator, plan 2
  support only single-bit range, instead of arbitrary
  NFS client generates single-block write RPCs, even for largefile

* Some performance

** running on ext3 over image in tmpfs

[tmp]$ rm nfs3.img 
[tmp]$ dd if=/dev/zero of=nfs3.img bs=4K count=100000
[tmp]$ mkfs -t ext3 nfs3.img
[tmp]$ sudo mount -t ext3 -o data=journal,sync -o loop /tmp/nfs3.img /srv/nfs/bench
[tmp]$ sudo systemctl start nfs-server.service
[tmp]$ sudo mount -t nfs -o vers=3 localhost:/srv/nfs/bench /mnt/nfs
[tmp]$ sudo chmod 777 /srv/nfs/bench

[bench (master)]$ ~/hack/fscq-impl/bench/largefile /mnt/nfs
makefile 50 MB 295276 usec throughput 173397.1 KB/s
writefile 50 MB 168222 usec throughput 304359.7 KB/s

** parallel lookup 

[goose-nfs (master)]$ go run ./cmd/lookup/main.go
Lookup: 385771 file in 1000000 usec with 1 threads
Lookup: 593760 file in 1000000 usec with 2 threads
Lookup: 771640 file in 1000000 usec with 3 threads
Lookup: 809043 file in 1000000 usec with 4 threads

** NULL RPC

[goose-nfs (master)]$ go run ./cmd/clnt-smallfile/main.go 
null: 33551 getattr in 1000000 usec

** <2020-01-24 Fri>: smallfile (including remove)

# Linux loopback
$ ./run-linux.sh  go run ./cmd/fs-smallfile/main.go
fs-smallfile: 3168.5 file/sec

# goose-nfsd over loopback
[goose-nfs (master)]$ ./run-goose-nfs.sh  go run ./cmd/fs-smallfile/main.go
2020/01/24 06:52:03 MkFsSuper: open file disk /dev/shm/goose.img
2020/01/24 06:52:04 NFS Null
2020/01/24 06:52:04 Null
2020/01/24 06:52:04 Null
2020/01/24 06:52:04 Mount /
2020/01/24 06:52:04 NFS Null
go run ./cmd/fs-smallfile/main.go
fs-smallfile: 2606.6 file/sec

# in-server goose-nfsd
[goose-nfs (master)]$ go run ./cmd/smallfile/main.go 
2020/01/24 06:54:43 MkFsSuper: open file disk /dev/shm/goose4d65822107fcfd52.img
smallfile: 8443.1 file/swith 1 threads

# NFS client

[goose-nfs (master)]$ ./run-goose-clnt.sh  go run ./cmd/clnt-smallfile/main.go 
2020/01/24 07:01:49 MkFsSuper: open file disk /dev/shm/goose.img
go run ./cmd/clnt-smallfile/main.go
2020/01/24 07:01:50 Mount /
root fh {[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]}
clnt-smallfile: 2784.5 file/s

